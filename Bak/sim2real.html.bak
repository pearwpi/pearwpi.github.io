	<!DOCTYPE html>
	<html lang="zxx" class="no-js">
	<head>
		<!-- Mobile Specific Meta -->
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<!-- Favicon-->
		<link rel="shortcut icon" href="img/fav.png">
		<!-- Author Meta -->
		<meta name="author" content="colorlib">
		<!-- Meta Description -->
		<meta name="description" content="">
		<!-- Meta Keyword -->
		<meta name="keywords" content="">
		<!-- meta character set -->
		<meta charset="UTF-8">
		<!-- Site Title -->
		<title>PeAR WPI</title>
		<!-- Site Title -->
		<!-- Site Title -->

		<link href="https://fonts.googleapis.com/css?family=Poppins:100,200,400,300,500,600,700" rel="stylesheet"> 
			<!--
			CSS
			============================================= -->
			<link rel="stylesheet" href="css/linearicons.css">
			<link rel="stylesheet" href="css/font-awesome.min.css">
			<link rel="stylesheet" href="css/bootstrap.css">
			<link rel="stylesheet" href="css/magnific-popup.css">			
			<link rel="stylesheet" href="css/nice-select.css">							
			<link rel="stylesheet" href="css/animate.min.css">
			<link rel="stylesheet" href="css/owl.carousel.css">			
			<link rel="stylesheet" href="css/jquery-ui.css">			
			<link rel="stylesheet" href="css/main.css">
			<link href="css/icofont/icofont.min.css" rel="stylesheet">
		    <link href="css/remixicon/remixicon.css" rel="stylesheet">
		    <link href="css/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
		    <link href="css/boxicons/css/boxicons.min.css" rel="stylesheet">
		    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
		    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
		    <!-- Global site tag (gtag.js) - Google Analytics -->
			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171009851-1"></script>
			<script>
			  window.dataLayer = window.dataLayer || [];
			  function gtag(){dataLayer.push(arguments);}
			  gtag('js', new Date());

			  gtag('config', 'UA-171009851-1');
			</script>
		</head>
		<body>	
		<!-- EDIT ME -->
			<header id="header">
			<div class="container main-menu">
			    <div class="row align-items-center justify-content-between d-flex"> 
			      <!--  style="margin-left: -36vh; margin-right: -36vh" -->
			      <div id="logo">
			        <a href="index.html" style="font-size: 24px; font-weight: 800"><img src="img/logos/1.png" width="48px" alt="" title=""> Perception and Autonomous Robotics Group</a>
			      </div>
			      <nav id="nav-menu-container">
			        <ul class="nav-menu">
			          <li><a title="Home" href="index.html" style="position: relative; top: -4px"><i style="font-size: 28px" class="fa fa-home"></i></a></li>
			          <li class="menu-has-children"><a title="Research" href="research.html">Research</a>
			            <ul>
			              <li><a href="research.html">Research Areas</a></li>
			              <!-- <li><a href="softwares.html">Softwares/Datasets</a></li> -->
			              <li><a href="publications.html">Publications/Softwares/Datasets</a></li>
			              <li><a href="labs.html">Labs/Facilities</a></li>
			            </ul>
			          </li> 
			          <li><a title="Teaching" href="teaching.html">Teaching</a></li>
			         <li><a title="Media" href="media.html">Media</a></li>
			         <li><a title="Openings" href="openings.html">Openings</a></li>
			         <li><a title="Events" href="events.html">Events</a></li>
			          </ul>
			        </nav><!-- #nav-menu-container -->                  
			    </div>
			</div>
			</header>   			
			<!-- EDIT ME -->




				<!-- Start Sample Area -->
			<section class="sample-text-area">
				<div class="container">
					<h3 class="text-heading">Sim2Real And Real2Sim</h3>
					<p class="sample-text">
						We work extensively on the perception stack for autonomous robots. Most of these perception modules are made of deep learning and require high quality and large amounts of data which are often not possible. To this end, we propose methods to leverage simulation engines for automatic high-fidelity data generation.
					</p>
				</div>
			</section>
			<!-- End Sample Area -->
			<!-- Start Align Area -->
			<div class="whole-wrap">
				<div class="container">
					<div class="section-top-border" style="text-align: justify">

							<h3 class="mb-30">Olive The Above</h3>
							Modern robotics has enabled the advancement in yield estimation for precision agriculture. However, when applied to the olive industry, the high variation of olive colors and their similarity to the background leaf canopy presents a challenge. Labeling several thousands of very dense olive grove images for segmentation is a labor-intensive task. This paper presents a novel approach to detecting olives without the need to manually label data. In this work, we present the world’s first olive detection dataset comprised of synthetic and real olive tree images. This is accomplished by generating an auto-labeled photorealistic 3D model of an olive tree. Its geometry is then simplified for lightweight rendering purposes. In addition, experiments are conducted with a mix of synthetically generated and real images, yielding an improvement of up to 66% compared to when only using a small sample of real data. When access to real, human-labeled data is limited, a combination of mostly synthetic data and a small amount of real data can enhance olive detection.<br><br>

							<h3> References</h3><br>
							<div class="rowunmod">
							<div class="col-lg-6 col-md-6 mt-sm-20 left-align-p" style="padding-left:0; padding-right:0">
								<h4>Detecting Olives with Synthetic or Real Data? Olive the Above</h4><br>
								<div class="highlight-sec">
									<h6>IROS 2023</h6>
								</div>			
								<p>
									Yianni Karabatis, Xiaomin Lin, <b>Nitin J. Sanket</b>, Michail G. Lagoudakis, Yiannis Aloimonos, <i>IEEE International Conference on Intelligent Robots and Systems (IROS)</i>, 2023.<br><br>
								</p>

								<h6>
									 <a href="http://umd.edu"><i class="fa fa-map-marker"></i>&nbsp;UMD&nbsp;&nbsp;</a>   <a href="http://wpi.edu"><i class="fa fa-map-marker"></i>&nbsp;WPI&nbsp;&nbsp;</a> <br><br>
									 <!-- <a href="https://arxiv.org/abs/2210.00715"><i class="fa fa-file-text-o"></i>&nbsp;Paper&nbsp;&nbsp;</a> -->
									<!-- <a href="research/2023/worldgen.html"><i class="fa fa-quote-right"></i>&nbsp;Cite&nbsp;&nbsp;</a>  -->
								</h6>
							</div>
							<div class="col-lg-6 col-md-6 mt-sm-20 right-align-p">
								<img src="img/research/2023/olivenet.png" alt="" class="img-fluid" style="border-radius: 16px;">
							</div>
						</div>
						<hr>


						<h3 class="mb-30">WorldGen</h3>

							In the era of deep learning, data is the critical determining factor in the performance of neural network models. Generating large datasets suffers from various difficulties such as scalability, cost efficiency and photorealism. To avoid expensive and strenuous dataset collection and annotations, researchers have inclined towards computer-generated datasets. Although, a lack of photorealism and a limited amount of computer-aided data, has bounded the accuracy of network predictions.<br><br>

							To this end, we present WorldGen - an open source framework to autonomously generate countless structured and unstructured 3D photorealistic scenes such as city view, object collection, and object fragmentation along with its rich ground truth annotation data. WorldGen being a generative model gives the user full access and control to features such as texture, object structure, motion, camera and lens properties for better generalizability by diminishing the data bias in the network. We demonstrate the effectiveness of WorldGen by presenting an evaluation on deep optical flow. We hope such a tool can open doors for future research in a myriad of domains related to robotics and computer vision by reducing manual labor and the cost of acquiring rich and high-quality data.<br><br>

							<h3> References</h3><br>

							<div class="rowunmod">
							<div class="col-lg-6 col-md-6 mt-sm-20 left-align-p" style="padding-left:0; padding-right:0">
								<h4><a href="https://arxiv.org/abs/2210.00715">WorldGen: A Large Scale Generative Simulator</h4></a><br>
								<div class="highlight-sec">
									<h6>ICRA 2023</h6>
								</div>			
								<p>
									Chahat Deep Singh, Riya Kumari, Cornelia Fermuller, <b>Nitin J. Sanket</b>, Yiannis Aloimonos, <i>IEEE International Conference on Robotics and Automation (ICRA)</i>, 2023.<br><br>
								</p>

								<h6>
									<a href="https://arxiv.org/abs/2210.00715"><i class="fa fa-file-text-o"></i>&nbsp;Paper&nbsp;&nbsp;</a><a href="https://prg.cs.umd.edu/WorldGen"><i class="fa fa-globe"></i>&nbsp;Project Page&nbsp;&nbsp;</a>  <a href="http://umd.edu"><i class="fa fa-map-marker"></i>&nbsp;UMD&nbsp;&nbsp;</a>   <a href="http://wpi.edu"><i class="fa fa-map-marker"></i>&nbsp;WPI&nbsp;&nbsp;</a> <br><br>
									<!-- <a href="research/2023/worldgen.html"><i class="fa fa-quote-right"></i>&nbsp;Cite&nbsp;&nbsp;</a>  -->
								</h6>
							</div>
							<div class="col-lg-6 col-md-6 mt-sm-20 right-align-p">
								<img src="img/research/2023/worldgen.png" alt="" class="img-fluid" style="border-radius: 16px;">
							</div>
						</div>
						<hr>

						<h3 class="mb-30">OysterNet</h3>
						Oysters play a pivotal role in the bay living ecosystem and are considered the living filters for the ocean. In recent years, oyster reefs have undergone major devastation caused by commercial over-harvesting, requiring preservation to maintain ecological balance. The foundation of this preservation is to estimate the oyster density which requires accurate oyster detection. However, systems for accurate oyster detection require large datasets obtaining which is an expensive and labor-intensive task in underwater environments.<br><br>

						To this end, we present a novel method to mathematically model oysters and render images of oysters in simulation to boost the detection performance with minimal real data. Utilizing our synthetic data along with real data for oyster detection, we obtain up to 35.1% boost in performance as compared to using only real data with our OysterNet network. We also improve the state-of-the-art by 12.7%. This shows that using underlying geometrical properties of objects can help to enhance recognition task accuracy on limited datasets successfully and we hope more researchers adopt such a strategy for hard-to-obtain datasets.<br><br>

						<h3> References</h3><br>

						<div class="rowunmod">
							<div class="col-lg-6 col-md-6 mt-sm-20 left-align-p" style="padding-left:0; padding-right:0">
								<h4><a href="https://arxiv.org/abs/2209.08176">Oysternet: Enhanced oyster detection using simulation</a></h4><br>
								<div class="highlight-sec">
									<h6>ICRA 2023</h6>
								</div>			
								<p>
									Xiaomin Lin, <b>Nitin J. Sanket</b>, Nare Karapetyan, Cornelia Fermuller, Yiannis Aloimonos, <i>IEEE International Conference on Robotics and Automation (ICRA)</i>, 2023.<br><br>
								</p>

								<h6>
									<a href="https://arxiv.org/abs/2209.08176"><i class="fa fa-file-text-o"></i>&nbsp;Paper&nbsp;&nbsp;</a><a href="https://prg.cs.umd.edu/OysterNet"><i class="fa fa-globe"></i>&nbsp;Project Page&nbsp;&nbsp;</a>  <a href="http://umd.edu"><i class="fa fa-map-marker"></i>&nbsp;UMD&nbsp;&nbsp;</a><a href="http://wpi.edu"><i class="fa fa-map-marker"></i>&nbsp;WPI&nbsp;&nbsp;</a>  <br><br>
									<!-- <a href="research/2023/oysternet.html"><i class="fa fa-quote-right"></i>&nbsp;Cite&nbsp;&nbsp;</a>  -->
								</h6>
							</div>
							<div class="col-lg-6 col-md-6 mt-sm-20 right-align-p">
								<img src="img/research/2023/oysternet.png" alt="" class="img-fluid" style="border-radius: 16px;">
							</div>
						</div>
						<hr>
						

                        <h3 class="mb-30">EVPropNet</h3>
                        The rapid rise of accessibility of unmanned aerial vehicles or drones pose a threat to general security and confidentiality. Most of the commercially available or custom-built drones are multi-rotors and are comprised of multiple propellers. Since these propellers rotate at a high-speed, they are generally the fastest moving parts of an image and cannot be directly "seen" by a classical camera without severe motion blur. We utilize a class of sensors that are particularly suitable for such scenarios called event cameras, which have a high temporal resolution, low-latency, and high dynamic range.<br><br>

						In this paper, we model the geometry of a propeller and use it to generate simulated events which are used to train a deep neural network called EVPropNet to detect propellers from the data of an event camera. EVPropNet directly transfers to the real world without any fine-tuning or retraining. We present two applications of our network: (a) tracking and following an unmarked drone and (b) landing on a near-hover drone. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with different propeller shapes and sizes. Our network can detect propellers at a rate of 85.1% even when 60% of the propeller is occluded and can run at upto 35Hz on a 2W power budget. To our knowledge, this is the first deep learning-based solution for detecting propellers (to detect drones). Finally, our applications also show an impressive success rate of 92% and 90% for the tracking and landing tasks respectively.<br><br>

						<h3> References</h3><br>

                        <div class="rowunmod">
                            <div class="col-lg-6 col-md-6 mt-sm-20 left-align-p" style="padding-left:0; padding-right:0">
                                <h4><a href="https://arxiv.org/abs/2106.15045">EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following</a></h4><br>
                                <div class="highlight-sec">
                                    <h6>RSS 2021</h6>
                                </div>          
                                <p>
                                <b>Nitin J. Sanket</b>, Chahat Deep Singh, Chethan M. Parameshwara, Cornelia Fermuller, Guido C.H.E. de Croon, Yiannis Aloimonos, <i>Robotics Science and Systems (RSS)</i>, 2021.<br>
                                </p>
                                 <h6>
                                    <a href="https://arxiv.org/abs/2106.15045"><i class="fa fa-file-text-o"></i>&nbsp;Paper&nbsp;&nbsp;</a> <a href="http://prg.cs.umd.edu/EVPropNet"><i class="fa fa-globe"></i>&nbsp;Project Page&nbsp;&nbsp;</a> <a href="https://github.com/prgumd/EVPropNet"><i class="fa fa-github"></i>&nbsp;Code&nbsp;&nbsp;</a> <a href="http://umd.edu"><i class="fa fa-map-marker"></i>&nbsp;UMD&nbsp;&nbsp;</a> 

                                    <!-- <a href="research/evpropnet.html"><i class="fa fa-quote-right"></i>&nbsp;Cite&nbsp;&nbsp;</a>  -->
                                </h6> 
                            </div>
                            <div class="col-lg-6 col-md-6 mt-sm-20 right-align-p">
                                <img src="img/research/evpropnet.png" alt="" class="img-fluid" style="border-radius: 16px;">
                            </div>
                        </div>
                        <hr>


						<h3 class="mb-30">PRGFlow</h3>
						Odometry on aerial robots has to be of low latency and high robustness whilst also respecting the Size, Weight, Area and Power (SWAP) constraints as demanded by the size of the robot. A combination of visual sensors coupled with Inertial Measurement Units (IMUs) has proven to be the best combination to obtain robust and low latency odometry on resource-constrained aerial robots. Recently, deep learning approaches for Visual Inertial fusion have gained momentum due to their high accuracy and robustness. However, the remarkable advantages of these techniques are their inherent scalability (adaptation to different sized aerial robots) and unification (same method works on different sized aerial robots) by utilizing compression methods and hardware acceleration, which have been lacking from previous approaches. To this end, we present a deep learning approach for visual translation estimation and loosely fuse it with an Inertial sensor for full 6 DoF odometry estimation. We also present a detailed benchmark comparing different architectures, loss functions and compression methods to enable scalability. We evaluate our network on the MSCOCO dataset and evaluate the VI fusion on multiple real-flight trajectories.<br><br>

						<h3> References</h3><br>

						<div class="rowunmod">
							<div class="col-lg-6 col-md-6 mt-sm-20 left-align-p" style="padding-left:0; padding-right:0">
								<h4><a href="https://arxiv.org/abs/2006.06753" style="font-weight: 600;"> PRGFlow: Unified SWAP‐aware deep global optical flow for aerial robot navigation</a></h4><br>
								<div class="highlight-sec">
									<h6>Electronic Letters 2021</h6>
								</div>			
								<p>
								<b>Nitin J. Sanket</b>, Chahat Deep Singh, Cornelia Fermuller, Yiannis Aloimonos, <i>Electronics Letters</i>, 2021.<br>
								</p>
								<h6>
									<a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ell2.12274"><i class="fa fa-file-text-o"></i>&nbsp;Paper&nbsp;&nbsp;</a> <a href="http://prg.cs.umd.edu/PRGFlow"><i class="fa fa-globe"></i>&nbsp;Project Page&nbsp;&nbsp;</a> <a href="https://github.com/prgumd/PRGFlow"><i class="fa fa-github"></i>&nbsp;Code&nbsp;&nbsp;</a> <a href="http://umd.edu"><i class="fa fa-map-marker"></i>&nbsp;UMD&nbsp;&nbsp;</a> 

									<!-- <a href="research/prgflow.html"><i class="fa fa-quote-right"></i>&nbsp;Cite&nbsp;&nbsp;</a>  -->
								</h6>
							</div>
							<div class="col-lg-6 col-md-6 mt-sm-20 right-align-p">
								<img src="img/research/PRGFlow.png" alt="" class="img-fluid" style="border-radius: 16px;">
							</div>
						</div>
						<hr>

						
						<h3 class="mb-30">EVDodgeNet</h3>
						Dynamic obstacle avoidance on quadrotors requires low latency. A class of sensors that are particularly suitable for such scenarios are event cameras. In this paper, we present a deep learning based solution for dodging multiple dynamic obstacles on a quadrotor with a single event camera and onboard computation. Our approach uses a series of shallow neural networks for estimating both the ego-motion and the motion of independently moving objects. The networks are trained in simulation and directly transfer to the real world without any fine-tuning or retraining. We successfully evaluate and demonstrate the proposed approach in many real-world experiments with obstacles of different shapes and sizes, achieving an overall success rate of 70% including objects of unknown shape and a low light testing scenario. To our knowledge, this is the first deep learning based solution to the problem of dynamic obstacle avoidance using event cameras on a quadrotor. Finally, we also extend our work to the pursuit task by merely reversing the control policy, proving that our navigation stack can cater to different scenarios.<br><br>

						<h3> References</h3><br>

						<div class="rowunmod">
							<div class="col-lg-6 col-md-6 mt-sm-20 left-align-p" style="padding-left:0; padding-right:0">
								<h4><a href="https://arxiv.org/abs/1906.02919" style="font-weight: 600;"> EVDodgeNet: Deep Dynamic Obstacle Dodging with Event Cameras</a></h4><br>
								<div class="highlight-sec">
									<h6>ICRA 2020</h6>
								</div>	
								<p>
								<b>Nitin J. Sanket*</b>, Chethan M. Parameshwara*, Chahat Deep Singh, Ashwin V. Kuruttukulam, Cornelia Fermuller, Davide Scaramuzza, Yiannis Aloimonos, <i>IEEE International Confernce on Robotics and Automation</i>, Paris, 2020.<br>
								* Equal Contribution

								<!-- Add text background in p tag with div -->
								
								</p>
								<h6>
									<a href="https://arxiv.org/abs/1906.02919"><i class="fa fa-file-text-o"></i>&nbsp;Paper&nbsp;&nbsp;</a> <a href="http://prg.cs.umd.edu/EVDodgeNet"><i class="fa fa-globe"></i>&nbsp;Project Page&nbsp;&nbsp;</a> <a href="https://github.com/prgumd/EVDodgeNet"><i class="fa fa-github"></i>&nbsp;Code&nbsp;&nbsp;</a> <a href="http://umd.edu"><i class="fa fa-map-marker"></i>&nbsp;UMD&nbsp;&nbsp;</a>  <br><br>

									<!-- <a href="research/evdodgenet.html"><i class="fa fa-quote-right"></i>&nbsp;Cite&nbsp;&nbsp;</a>  -->

									<h4>Featured in</h4> <br>
									<a href="https://mashable.com/video/drone-uses-ai-to-dodge-objects-thrown-at-it/"><img src="img/logos/Mashable.png" width="140px" alt="" class="img-fluid"></a> <a href="https://futurism.com/the-byte/watch-drones-dodge-stuff-thrown"><img src="img/logos/Futurism.png" width="140px" alt="" class="img-fluid"></a>
								</h6>
							</div>
							<div class="col-lg-6 col-md-6 mt-sm-20 right-align-p">
								<img src="img/research/EVDodgeNet.gif" alt="" class="img-fluid" style="border-radius: 16px;">
							</div>
						</div>
						<br><br>
						<hr>

					</div>
				</div>
			</div>


            <!-- EDIT FOOT -->
            	<!-- start footer Area -->
            	            <section class="facts-area section-gap" id="facts-area"  style="background-color: rgba(255, 255, 255, 1.0); padding: 40px">
            	                <div class="container">     
            	                <div class="title text-center">
            	                        <p> <a href="index.html"><img src="img/logos/1.png" width="128px" alt="" title=""></a><br><br>
            	                            Perception and Autonomous Robotics Group <br>
            	                            Worcester Polytechnic Institute <br>
            	                            Copyright © 2023<br>
            	                            <span style="font-size: 10px">Website based on <a href="https://colorlib.com" target="_blank">Colorlib</a></span>
            	                        </p>
            	                    </div>
            	                </div>  
            	            </section>  
            	<!-- End footer Area -->            <!-- EDIT FOOT -->	

			<script src="js/vendor/jquery-2.2.4.min.js"></script>
			<script src="js/popper.min.js"></script>
			<script src="js/vendor/bootstrap.min.js"></script>			
			<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBhOdIF3Y9382fqJYt5I_sswSrEw5eihAA"></script>			
  			<script src="js/easing.min.js"></script>			
			<script src="js/hoverIntent.js"></script>
			<script src="js/superfish.min.js"></script>	
			<script src="js/jquery.ajaxchimp.min.js"></script>
			<script src="js/jquery.magnific-popup.min.js"></script>	
    		<script src="js/jquery.tabs.min.js"></script>						
			<script src="js/jquery.nice-select.min.js"></script>	
            <script src="js/isotope.pkgd.min.js"></script>			
			<script src="js/waypoints.min.js"></script>
			<script src="js/jquery.counterup.min.js"></script>
			<script src="js/simple-skillbar.js"></script>							
			<script src="js/owl.carousel.min.js"></script>							
			<script src="js/mail-script.js"></script>	
			<script src="js/main.js"></script>	
		</body>
	</html>
